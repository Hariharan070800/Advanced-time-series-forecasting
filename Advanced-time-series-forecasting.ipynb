{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85eaddf-7b5d-4513-8bf2-5b1008c0911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Advanced Time Series Forecasting with LSTM and Attention\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_multivariate_series(\n",
    "    n_samples: int = 3000,\n",
    "    noise_level: float = 0.2\n",
    ") -> pd.DataFrame:\n",
    "    t = np.arange(n_samples)\n",
    "\n",
    "    f1 = np.sin(0.02 * t)\n",
    "    f2 = np.cos(0.015 * t)\n",
    "    f3 = np.sin(0.01 * t) + 0.5 * f1\n",
    "    f4 = 0.3 * f2 + np.sin(0.05 * t)\n",
    "    f5 = np.sin(0.03 * t) + np.cos(0.02 * t)\n",
    "    f6 = f1 * f2\n",
    "\n",
    "    noise = noise_level * np.random.randn(n_samples, 6)\n",
    "\n",
    "    data = np.vstack([f1, f2, f3, f4, f5, f6]).T + noise\n",
    "\n",
    "    columns = [f\"feature_{i}\" for i in range(6)]\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "def create_sequences(\n",
    "    data: np.ndarray,\n",
    "    seq_length: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert time series into supervised learning sequences.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length, 0])  # Predict feature_0\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "def build_baseline_lstm(\n",
    "    seq_length: int,\n",
    "    n_features: int,\n",
    "    hidden_units: int = 64\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Build standard LSTM forecasting model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_length, n_features))\n",
    "    x = layers.LSTM(hidden_units)(inputs)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "\n",
    "    return model\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Implements Bahdanau additive attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def _init_(self, units: int):\n",
    "        super()._init_()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        score = tf.nn.tanh(self.W1(hidden_states))\n",
    "        attention_weights = tf.nn.softmax(\n",
    "            self.V(score), axis=1\n",
    "        )\n",
    "        context_vector = attention_weights * hidden_states\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "def build_attention_lstm(\n",
    "    seq_length: int,\n",
    "    n_features: int,\n",
    "    hidden_units: int = 64\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Build LSTM with Bahdanau Attention.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_length, n_features))\n",
    "    lstm_out = layers.LSTM(\n",
    "        hidden_units,\n",
    "        return_sequences=True\n",
    "    )(inputs)\n",
    "\n",
    "    attention_layer = BahdanauAttention(hidden_units)\n",
    "    context_vector, attention_weights = attention_layer(lstm_out)\n",
    "\n",
    "    outputs = layers.Dense(1)(context_vector)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "\n",
    "    model.attention_layer = attention_layer\n",
    "\n",
    "    return model\n",
    "def rolling_origin_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    n_splits: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator for rolling-origin evaluation.\n",
    "    \"\"\"\n",
    "    fold_size = len(X) // (n_splits + 1)\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        train_end = fold_size * (i + 1)\n",
    "        val_end = fold_size * (i + 2)\n",
    "\n",
    "        yield (\n",
    "            X[:train_end],\n",
    "            y[:train_end],\n",
    "            X[train_end:val_end],\n",
    "            y[train_end:val_end]\n",
    "        )\n",
    "def train_and_evaluate(\n",
    "    model_builder,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    seq_length: int,\n",
    "    n_features: int\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train model using rolling validation and compute average RMSE and MAE.\n",
    "    \"\"\"\n",
    "    rmses, maes = [], []\n",
    "\n",
    "    for X_train, y_train, X_val, y_val in rolling_origin_splits(X, y):\n",
    "\n",
    "        model = model_builder(seq_length, n_features)\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=20,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "\n",
    "        rmses.append(rmse)\n",
    "        maes.append(mae)\n",
    "\n",
    "    return np.mean(rmses), np.mean(maes)\n",
    "if _name_ == \"_main_\":\n",
    "\n",
    "    df = generate_multivariate_series()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df)\n",
    "\n",
    "    SEQ_LENGTH = 50\n",
    "\n",
    "    X, y = create_sequences(scaled, SEQ_LENGTH)\n",
    "\n",
    "    baseline_rmse, baseline_mae = train_and_evaluate(\n",
    "        build_baseline_lstm,\n",
    "        X, y,\n",
    "        SEQ_LENGTH,\n",
    "        X.shape[2]\n",
    "    )\n",
    "\n",
    "    attention_rmse, attention_mae = train_and_evaluate(\n",
    "        build_attention_lstm,\n",
    "        X, y,\n",
    "        SEQ_LENGTH,\n",
    "        X.shape[2]\n",
    "    )\n",
    "\n",
    "    print(\"Baseline LSTM\")\n",
    "    print(\"RMSE:\", baseline_rmse)\n",
    "    print(\"MAE :\", baseline_mae)\n",
    "\n",
    "    print(\"\\nAttention LSTM\")\n",
    "    print(\"RMSE:\", attention_rmse)\n",
    "    print(\"MAE :\", attention_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
